{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982ac003",
   "metadata": {},
   "source": [
    "# Language Confidence Score\n",
    "\n",
    "Cel: majac tylko `word_counts` (slownik `word -> count`) oraz liste najczestszych slow w jezyku (`word -> frequency`), wyznaczyc wynik dopasowania tekstu do jezyka.\n",
    "\n",
    "Wymagane dane do eksperymentu (5 tekstow):\n",
    "- `wiki_long` (z wiki, 5000+ slow)\n",
    "- `wiki_short_bad` (z wiki, 20+ slow, jak najgorzej dopasowany do jezyka wiki)\n",
    "- `ext_<lang>` (dluzszy tekst spoza wiki dla kazdego z 3 jezykow)\n",
    "\n",
    "Wymagane jezyki: 3 (jezyk wybranej wiki + 2 inne).\n",
    "\n",
    "Zrodlo listy najczestszych slow: `wordfreq` (min. 1000 slow na jezyk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfdb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "_cwd = Path.cwd().resolve()\n",
    "_root = None\n",
    "for _p in [_cwd, *_cwd.parents]:\n",
    "    if (_p / 'wiki_scraper').is_dir() and (_p / 'wiki_scraper' / '__init__.py').exists():\n",
    "        _root = _p\n",
    "        break\n",
    "if _root is None:\n",
    "    raise RuntimeError('Could not find project root containing wiki_scraper/')\n",
    "if str(_root) not in sys.path:\n",
    "    sys.path.insert(0, str(_root))\n",
    "\n",
    "DATA_DIR = _root / 'data'\n",
    "print('cwd:', _cwd)\n",
    "print('root:', _root)\n",
    "print('DATA_DIR:', DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from wiki_scraper.words import tokenize_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba06f8",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "1. Pliki:\n",
    "- `data/wiki_long.json`\n",
    "- `data/wiki_short_bad.json`\n",
    "\n",
    "2. Teksty zewnetrzne:\n",
    "- `data/ext_en.txt`\n",
    "- `data/ext_pl.txt`\n",
    "- `data/ext_de.txt`\n",
    "\n",
    "Jezyki w tym notatniku: `en`, `pl`, `de`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89833b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = ['en', 'pl', 'de']\n",
    "K_VALUES = [3, 10, 100, 1000]\n",
    "\n",
    "PATH_WIKI_LONG = DATA_DIR / 'wiki_long.json'\n",
    "PATH_WIKI_SHORT_BAD = DATA_DIR / 'wiki_short_bad.json'\n",
    "\n",
    "PATH_EXT = {\n",
    "    'en': DATA_DIR / 'ext_en.txt',\n",
    "    'pl': DATA_DIR / 'ext_pl.txt',\n",
    "    'de': DATA_DIR / 'ext_de.txt',\n",
    "}\n",
    "\n",
    "print('wiki_long exists:', PATH_WIKI_LONG.exists())\n",
    "print('wiki_short_bad exists:', PATH_WIKI_SHORT_BAD.exists())\n",
    "for k, v in PATH_EXT.items():\n",
    "    print('ext', k, '->', v, 'exists:', v.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b563eac",
   "metadata": {},
   "source": [
    "## Ladowanie danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d14d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_counts_json(path: Path) -> dict[str, int]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Missing file: {path}. Create it from wiki_scraper.py --count-words.')\n",
    "    data = json.loads(path.read_text(encoding='utf-8'))\n",
    "    if not isinstance(data, dict):\n",
    "        raise ValueError(f'Invalid JSON object in {path}')\n",
    "    out: dict[str, int] = {}\n",
    "    for k, v in data.items():\n",
    "        if isinstance(k, str) and isinstance(v, int):\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def counts_from_text_file(path: Path) -> dict[str, int]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Missing file: {path}. Provide a longer external text.')\n",
    "    text = path.read_text(encoding='utf-8', errors='replace')\n",
    "    return dict(Counter(tokenize_words(text)))\n",
    "\n",
    "def total_words(counts: dict[str, int]) -> int:\n",
    "    return int(sum(counts.values()))\n",
    "\n",
    "wiki_long = load_word_counts_json(PATH_WIKI_LONG)\n",
    "wiki_short_bad = load_word_counts_json(PATH_WIKI_SHORT_BAD)\n",
    "\n",
    "ext_counts: dict[str, dict[str, int]] = {}\n",
    "for lang in LANGUAGES:\n",
    "    ext_counts[lang] = counts_from_text_file(PATH_EXT[lang])\n",
    "\n",
    "datasets: dict[str, dict[str, int]] = {\n",
    "    'wiki_long': wiki_long,\n",
    "    'wiki_short_bad': wiki_short_bad,\n",
    "}\n",
    "for lang, wc in ext_counts.items():\n",
    "    datasets[f'ext_{lang}'] = wc\n",
    "\n",
    "pd.DataFrame(\n",
    "    [{'dataset': name, 'total_words': total_words(wc), 'unique_words': len(wc)} for name, wc in datasets.items()]\n",
    ").sort_values('dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert total_words(datasets['wiki_long']) >= 5000, 'wiki_long must be 5000+ words'\n",
    "assert total_words(datasets['wiki_short_bad']) >= 20, 'wiki_short_bad must be 20+ words'\n",
    "'OK'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1994f64a",
   "metadata": {},
   "source": [
    "## Dane jezykowe (wordfreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe614db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import top_n_list, word_frequency\n",
    "\n",
    "def get_language_frequency_list(language_code: str, n: int) -> list[tuple[str, float]]:\n",
    "    words = top_n_list(language_code, n)\n",
    "    pairs = [(w, float(word_frequency(w, language_code))) for w in words]\n",
    "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return pairs\n",
    "\n",
    "LANG_LIST_SIZE = 5000\n",
    "language_lists: dict[str, list[tuple[str, float]]] = {\n",
    "    lang: get_language_frequency_list(lang, LANG_LIST_SIZE) for lang in LANGUAGES\n",
    "}\n",
    "{lang: len(lst) for lang, lst in language_lists.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5bf7ed",
   "metadata": {},
   "source": [
    "## Funkcja lang_confidence_score\n",
    "\n",
    "Cosine similarity miedzy rozkladem slow tekstu i rozkladem slow jezyka (top-k), oba znormalizowane i ograniczone do wspolnego vocab (top-k slow danego jezyka).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c31d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_confidence_score(\n",
    "    word_counts: dict[str, int],\n",
    "    language_words_with_frequency: list[tuple[str, float]],\n",
    ") -> float:\n",
    "    if not language_words_with_frequency:\n",
    "        return 0.0\n",
    "    vocab = [w for (w, _) in language_words_with_frequency if w]\n",
    "    lang_freq = {w: float(f) for (w, f) in language_words_with_frequency if w}\n",
    "    lang_sum = sum(lang_freq.values())\n",
    "    if lang_sum <= 0:\n",
    "        return 0.0\n",
    "    lang_vec = {w: lang_freq[w] / lang_sum for w in vocab}\n",
    "\n",
    "    text_sum = 0\n",
    "    text_raw: dict[str, float] = {}\n",
    "    for w in vocab:\n",
    "        c = int(word_counts.get(w, 0))\n",
    "        if c > 0:\n",
    "            text_raw[w] = float(c)\n",
    "            text_sum += c\n",
    "    if text_sum <= 0:\n",
    "        return 0.0\n",
    "    text_vec = {w: c / text_sum for w, c in text_raw.items()}\n",
    "\n",
    "    dot = 0.0\n",
    "    a2 = 0.0\n",
    "    b2 = 0.0\n",
    "    for w in vocab:\n",
    "        a = text_vec.get(w, 0.0)\n",
    "        b = lang_vec.get(w, 0.0)\n",
    "        dot += a * b\n",
    "        a2 += a * a\n",
    "        b2 += b * b\n",
    "    denom = math.sqrt(a2) * math.sqrt(b2)\n",
    "    return 0.0 if denom <= 0 else (dot / denom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e3bf4",
   "metadata": {},
   "source": [
    "## Wyniki dla k = 3, 10, 100, 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for k in K_VALUES:\n",
    "    for lang in LANGUAGES:\n",
    "        topk = language_lists[lang][:k]\n",
    "        for dataset_name, wc in datasets.items():\n",
    "            rows.append({\n",
    "                'k': k,\n",
    "                'language': lang,\n",
    "                'dataset': dataset_name,\n",
    "                'score': lang_confidence_score(wc, topk),\n",
    "            })\n",
    "results = pd.DataFrame(rows)\n",
    "results.sort_values(['dataset', 'k', 'language']).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12188018",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = results.pivot_table(index=['dataset', 'k'], columns='language', values='score')\n",
    "pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8798f",
   "metadata": {},
   "source": [
    "## Wykresy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e692dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "charts_dir = DATA_DIR / 'charts'\n",
    "charts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for dataset_name in sorted(datasets.keys()):\n",
    "    sub = results[results['dataset'] == dataset_name].copy()\n",
    "    fig, ax = plt.subplots(figsize=(10, 4.5))\n",
    "    for lang in LANGUAGES:\n",
    "        s = sub[sub['language'] == lang].sort_values('k')\n",
    "        ax.plot(s['k'], s['score'], marker='o', label=lang)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_title(f'lang_confidence_score vs k ({dataset_name})')\n",
    "    ax.set_xlabel('k (log scale)')\n",
    "    ax.set_ylabel('score')\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend()\n",
    "\n",
    "    out_path = charts_dir / f'{dataset_name}.png'\n",
    "    fig.savefig(out_path, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "print('saved charts to:', charts_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fac765",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
