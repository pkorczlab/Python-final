{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76326747",
      "metadata": {},
      "source": [
        "# Language Confidence Score (Word-Frequency Matching)\n",
        "\n",
        "Cel: majac tylko `word_counts` (slownik word -> count) oraz liste najczestszych slow w jezyku (word -> frequency), wyznaczyc wynik dopasowania tekstu do jezyka.\n",
        "\n",
        "Wymagane dane do eksperymentu (5 tekstow):\n",
        "- `wiki_long` (z wiki, 5000+ slow)\n",
        "- `wiki_short_bad` (z wiki, 20+ slow, jak najgorzej dopasowany do jezyka wiki)\n",
        "- `ext_<lang>` (dluzszy tekst spoza wiki dla kazdego z 3 jezykow)\n",
        "\n",
        "Wymagane jezyki: 3 (jezyk wybranej wiki + 2 inne).\n",
        "\n",
        "Zrodlo listy najczestszych slow: `wordfreq` (min. 1000 slow na jezyk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cc99e07c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cwd: /home/piotr/Dokumenty/python/wck-backup/notebooks\n",
            "python: /home/piotr/Dokumenty/python/wck-backup/venv/bin/python3\n",
            "root: /home/piotr/Dokumenty/python/wck-backup\n",
            "has wiki_scraper dir: True\n",
            "sys.path[0:3]: ['/home/piotr/Dokumenty/python/wck-backup', '/home/piotr/Dokumenty/python/wck-backup/notebooks', '/home/piotr/Dokumenty/python/wck-backup/notebooks']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure project root (folder containing wiki_scraper/) is importable\n",
        "_cwd = Path.cwd().resolve()\n",
        "_root = None\n",
        "for _p in [_cwd, *_cwd.parents]:\n",
        "    if (_p / 'wiki_scraper').is_dir() and (_p / 'wiki_scraper' / '__init__.py').exists():\n",
        "        _root = _p\n",
        "        break\n",
        "if _root is None:\n",
        "    raise RuntimeError('Could not find project root containing wiki_scraper/')\n",
        "if str(_root) not in sys.path:\n",
        "    sys.path.insert(0, str(_root))\n",
        "\n",
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"cwd:\", os.getcwd())\n",
        "print(\"python:\", sys.executable)\n",
        "\n",
        "# znajdz katalog projektu (taki, ktory zawiera folder wiki_scraper/)\n",
        "p = Path.cwd().resolve()\n",
        "root = None\n",
        "for parent in [p, *p.parents]:\n",
        "  if (parent / \"wiki_scraper\").is_dir() and (parent / \"wiki_scraper\" / \"__init__.py\").exists():\n",
        "      root = parent\n",
        "      break\n",
        "\n",
        "print(\"root:\", root)\n",
        "print(\"has wiki_scraper dir:\", (root / \"wiki_scraper\").is_dir() if root else None)\n",
        "\n",
        "if root and str(root) not in sys.path:\n",
        "  sys.path.insert(0, str(root))\n",
        "\n",
        "print(\"sys.path[0:3]:\", sys.path[:3])\n",
        "\n",
        "\n",
        "#from wiki_scraper.words import tokenize_words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eef09a7",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Konfiguracja danych\n",
        "\n",
        "Ponizej ustaw sciezki do danych.\n",
        "\n",
        "Jak je utworzyc:\n",
        "- Dla wiki: uruchom `python3 wiki_scraper.py --count-words \"Tytul\"`, potem skopiuj `word-counts.json` do `data/wiki_long.json` i `data/wiki_short_bad.json` (za kazdym razem kasuj `word-counts.json`, zeby nie sumowalo sie miedzy tekstami).\n",
        "- Dla tekstow zewnetrznych: wklej dlugie teksty do `data/ext_en.txt`, `data/ext_pl.txt`, `data/ext_es.txt` (lub inne jezyki)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c475e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "LANGUAGES = [\n",
        "    # Jezyk wiki (Bulbapedia):\n",
        "    'en',\n",
        "    # Dwa inne jezyki do porownania:\n",
        "    'pl',\n",
        "    'es',\n",
        "]\n",
        "\n",
        "K_VALUES = [3, 10, 100, 1000]\n",
        "\n",
        "DATA_DIR = Path('data')\n",
        "PATH_WIKI_LONG = DATA_DIR / 'wiki_long.json'\n",
        "PATH_WIKI_SHORT_BAD = DATA_DIR / 'wiki_short_bad.json'\n",
        "\n",
        "# Teksty zewnetrzne (po jednym na jezyk).\n",
        "# Jesli nie masz ktoregos, mozesz podmienic sciezke albo dopisac plik.\n",
        "PATH_EXT = {\n",
        "    'en': DATA_DIR / 'ext_en.txt',\n",
        "    'pl': DATA_DIR / 'ext_pl.txt',\n",
        "    'es': DATA_DIR / 'ext_es.txt',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8194743f",
      "metadata": {},
      "source": [
        "## Ladowanie danych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c45745d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_word_counts_json(path: Path) -> dict[str, int]:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f'Missing file: {path}. Create it from wiki_scraper.py --count-words.')\n",
        "    data = json.loads(path.read_text(encoding='utf-8'))\n",
        "    if not isinstance(data, dict):\n",
        "        raise ValueError(f'Invalid JSON object in {path}')\n",
        "    out: dict[str, int] = {}\n",
        "    for k, v in data.items():\n",
        "        if isinstance(k, str) and isinstance(v, int):\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def counts_from_text_file(path: Path) -> dict[str, int]:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f'Missing file: {path}. Provide a longer external text.')\n",
        "    text = path.read_text(encoding='utf-8', errors='replace')\n",
        "    return dict(Counter(tokenize_words(text)))\n",
        "\n",
        "def total_words(counts: dict[str, int]) -> int:\n",
        "    return int(sum(counts.values()))\n",
        "\n",
        "wiki_long = load_word_counts_json(PATH_WIKI_LONG)\n",
        "wiki_short_bad = load_word_counts_json(PATH_WIKI_SHORT_BAD)\n",
        "\n",
        "ext_counts: dict[str, dict[str, int]] = {}\n",
        "for lang in LANGUAGES:\n",
        "    if lang not in PATH_EXT:\n",
        "        continue\n",
        "    ext_counts[lang] = counts_from_text_file(PATH_EXT[lang])\n",
        "\n",
        "datasets: dict[str, dict[str, int]] = {\n",
        "    'wiki_long': wiki_long,\n",
        "    'wiki_short_bad': wiki_short_bad,\n",
        "}\n",
        "for lang, wc in ext_counts.items():\n",
        "    datasets[f'ext_{lang}'] = wc\n",
        "\n",
        "pd.DataFrame(\n",
        "    [{'dataset': name, 'total_words': total_words(wc), 'unique_words': len(wc)} for name, wc in datasets.items()]\n",
        ").sort_values('dataset')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a37e9ec",
      "metadata": {},
      "source": [
        "Sprawdz wymogi:\n",
        "- `wiki_long` powinien miec 5000+ slow\n",
        "- `wiki_short_bad` powinien miec 20+ slow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc5d544",
      "metadata": {},
      "outputs": [],
      "source": [
        "assert total_words(datasets['wiki_long']) >= 5000, 'wiki_long must be 5000+ words'\n",
        "assert total_words(datasets['wiki_short_bad']) >= 20, 'wiki_short_bad must be 20+ words'\n",
        "'OK'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371becf7",
      "metadata": {},
      "source": [
        "## Dane jezykowe (najczestsze slowa + czestotliwosci)\n",
        "\n",
        "`wordfreq` daje nam liste slow (top_n_list) i funkcje czestotliwosci (word_frequency).\n",
        "\n",
        "Wymaganie: min. 1000 najczestszych slow dla kazdego jezyka."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5301e7a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordfreq import top_n_list, word_frequency\n",
        "\n",
        "def get_language_frequency_list(language_code: str, n: int) -> list[tuple[str, float]]:\n",
        "    words = top_n_list(language_code, n)\n",
        "    # word_frequency gives a non-negative frequency estimate\n",
        "    pairs = [(w, float(word_frequency(w, language_code))) for w in words]\n",
        "    # Ensure sorted (defensive):\n",
        "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "    return pairs\n",
        "\n",
        "LANG_LIST_SIZE = 5000\n",
        "language_lists: dict[str, list[tuple[str, float]]] = {\n",
        "    lang: get_language_frequency_list(lang, LANG_LIST_SIZE) for lang in LANGUAGES\n",
        "}\n",
        "{lang: len(lst) for lang, lst in language_lists.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b623ccec",
      "metadata": {},
      "source": [
        "## Funkcja `lang_confidence_score(word_counts, language_words_with_frequency)`\n",
        "\n",
        "Propozycja: cosine similarity pomiedzy:\n",
        "- rozkladem slow w tekscie (ograniczonym do slow z listy jezyka)\n",
        "- rozkladem slow jezyka (top-k)\n",
        "\n",
        "Interpretacja: im wiecej slow z tekstu pokrywa sie z najczestszymi slowami jezyka i im bardziej podobne sa proporcje, tym wyzszy wynik.\n",
        "\n",
        "Zastosowanie `k`: w eksperymentach przekazujemy `language_words_with_frequency[:k]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708edb30",
      "metadata": {},
      "outputs": [],
      "source": [
        "def lang_confidence_score(\n",
        "    word_counts: dict[str, int],\n",
        "    language_words_with_frequency: list[tuple[str, float]],\n",
        ") -> float:\n",
        "    \"\"\"Return a confidence score that the text matches the language.\n",
        "\n",
        "    Expected input: `language_words_with_frequency` is already truncated to top-k.\n",
        "    Score: cosine similarity between normalized text distribution (restricted to the top-k language vocab)\n",
        "    and normalized language distribution over the same vocab.\n",
        "\"\"\"\n",
        "\n",
        "    if not language_words_with_frequency:\n",
        "        return 0.0\n",
        "\n",
        "    vocab = [w for (w, _) in language_words_with_frequency if w]\n",
        "    lang_freq = {w: float(f) for (w, f) in language_words_with_frequency if w}\n",
        "\n",
        "    lang_sum = sum(lang_freq.values())\n",
        "    if lang_sum <= 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Normalize language vector\n",
        "    lang_vec = {w: lang_freq[w] / lang_sum for w in vocab}\n",
        "\n",
        "    # Normalize text vector over vocab\n",
        "    text_sum = 0\n",
        "    text_raw: dict[str, float] = {}\n",
        "    for w in vocab:\n",
        "        c = int(word_counts.get(w, 0))\n",
        "        if c > 0:\n",
        "            text_raw[w] = float(c)\n",
        "            text_sum += c\n",
        "\n",
        "    if text_sum <= 0:\n",
        "        return 0.0\n",
        "\n",
        "    text_vec = {w: c / text_sum for w, c in text_raw.items()}\n",
        "\n",
        "    dot = 0.0\n",
        "    a2 = 0.0\n",
        "    b2 = 0.0\n",
        "    for w in vocab:\n",
        "        a = text_vec.get(w, 0.0)\n",
        "        b = lang_vec.get(w, 0.0)\n",
        "        dot += a * b\n",
        "        a2 += a * a\n",
        "        b2 += b * b\n",
        "\n",
        "    denom = math.sqrt(a2) * math.sqrt(b2)\n",
        "    if denom <= 0:\n",
        "        return 0.0\n",
        "    return dot / denom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36de9a10",
      "metadata": {},
      "source": [
        "## Eksperyment: k = 3, 10, 100, 1000\n",
        "\n",
        "Dla kazdego k, dla kazdego jezyka, dla kazdego datasetu liczymy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0629e128",
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = []\n",
        "for k in K_VALUES:\n",
        "    for lang in LANGUAGES:\n",
        "        topk = language_lists[lang][:k]\n",
        "        for dataset_name, wc in datasets.items():\n",
        "            score = lang_confidence_score(wc, topk)\n",
        "            rows.append({\n",
        "                'k': k,\n",
        "                'language': lang,\n",
        "                'dataset': dataset_name,\n",
        "                'score': score,\n",
        "            })\n",
        "\n",
        "results = pd.DataFrame(rows)\n",
        "results.sort_values(['dataset', 'k', 'language']).head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34303717",
      "metadata": {},
      "outputs": [],
      "source": [
        "pivot = results.pivot_table(index=['dataset', 'k'], columns='language', values='score')\n",
        "pivot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a70d53",
      "metadata": {},
      "source": [
        "## Wykresy\n",
        "\n",
        "Dla kazdego datasetu: score vs k (logarytmicznie), osobna linia dla kazdego jezyka."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7af98548",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.rcParams['figure.dpi'] = 130\n",
        "\n",
        "for dataset_name in sorted(datasets.keys()):\n",
        "    sub = results[results['dataset'] == dataset_name].copy()\n",
        "    fig, ax = plt.subplots(figsize=(10, 4.5))\n",
        "    for lang in LANGUAGES:\n",
        "        s = sub[sub['language'] == lang].sort_values('k')\n",
        "        ax.plot(s['k'], s['score'], marker='o', label=lang)\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_title(f'lang_confidence_score vs k ({dataset_name})')\n",
        "    ax.set_xlabel('k (log scale)')\n",
        "    ax.set_ylabel('score')\n",
        "    ax.grid(alpha=0.25)\n",
        "    ax.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009d8681",
      "metadata": {},
      "source": [
        "## Podsumowanie rankingow\n",
        "\n",
        "Dla kazdego datasetu i k: pokazujemy jezyk z najwyzszym score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4f6580",
      "metadata": {},
      "outputs": [],
      "source": [
        "def best_language_table(results: pd.DataFrame) -> pd.DataFrame:\n",
        "    out_rows = []\n",
        "    for (dataset_name, k), group in results.groupby(['dataset', 'k']):\n",
        "        group = group.sort_values('score', ascending=False)\n",
        "        out_rows.append({\n",
        "            'dataset': dataset_name,\n",
        "            'k': int(k),\n",
        "            'best_language': group.iloc[0]['language'],\n",
        "            'best_score': float(group.iloc[0]['score']),\n",
        "            'second_language': group.iloc[1]['language'] if len(group) > 1 else None,\n",
        "            'second_score': float(group.iloc[1]['score']) if len(group) > 1 else None,\n",
        "        })\n",
        "    return pd.DataFrame(out_rows).sort_values(['dataset', 'k'])\n",
        "\n",
        "best_tbl = best_language_table(results)\n",
        "best_tbl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8647a201",
      "metadata": {},
      "source": [
        "## Dodatkowe metryki: pokrycie (overlap)\n",
        "\n",
        "Mierzymy, jaki procent slow (z tekstu) znajduje sie w top-k slow danego jezyka.\n",
        "To pomaga zinterpretowac jezyki z duza odmiana (np. polski), gdzie wiele form moze nie trafic w top-k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e60756b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def overlap_ratio(word_counts: dict[str, int], vocab: set[str]) -> float:\n",
        "    total = sum(word_counts.values())\n",
        "    if total <= 0:\n",
        "        return 0.0\n",
        "    in_vocab = 0\n",
        "    for w, c in word_counts.items():\n",
        "        if w in vocab:\n",
        "            in_vocab += int(c)\n",
        "    return in_vocab / total\n",
        "\n",
        "overlap_rows = []\n",
        "for k in K_VALUES:\n",
        "    for lang in LANGUAGES:\n",
        "        vocab = {w for (w, _) in language_lists[lang][:k]}\n",
        "        for dataset_name, wc in datasets.items():\n",
        "            overlap_rows.append({\n",
        "                'k': k,\n",
        "                'language': lang,\n",
        "                'dataset': dataset_name,\n",
        "                'overlap_ratio': overlap_ratio(wc, vocab),\n",
        "            })\n",
        "\n",
        "overlap_df = pd.DataFrame(overlap_rows)\n",
        "overlap_df.pivot_table(index=['dataset', 'k'], columns='language', values='overlap_ratio')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd378d38",
      "metadata": {},
      "source": [
        "## Opis wynikow (do wypelnienia)\n",
        "\n",
        "Ponizej wpisz wlasny opis po uruchomieniu notebooka na realnych danych.\n",
        "\n",
        "Pytania z polecenia:\n",
        "- Czy dobor jezykow mial duze znaczenie?\n",
        "- Czy po wartosciach (np. overlap/score) widac odmiane slow w jezyku (np. polski)?\n",
        "- Czy trudno bylo znalezc artykul z wiki, ktory minimalizuje score dla jezyka wiki? Czy to specyfika wiki?\n",
        "\n",
        "Wskazowka: skorzystaj z tabel `pivot`, `best_tbl` oraz `overlap_df`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (wck-backup)",
      "language": "python",
      "name": "wck-backup"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
