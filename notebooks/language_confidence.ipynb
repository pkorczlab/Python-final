{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Language Confidence Score\n",
        "\n",
        "This notebook implements a simple method for scoring how well a text's word-frequency profile matches a given language's common-word frequency list.\n",
        "\n",
        "Inputs:\n",
        "- `word_counts`: dict mapping word -> count (as produced by `--count-words`)\n",
        "- `language_words_with_frequency`: list of `(word, frequency)` sorted by frequency desc (at least 1000 words)\n",
        "\n",
        "The experiments below are self-contained and offline by default. If you have `wordfreq` installed, you can use real frequency lists via `wordfreq.top_n_list` and `wordfreq.word_frequency`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from wiki_scraper.words import tokenize_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lang_confidence_score(\n",
        "    word_counts: dict[str, int],\n",
        "    language_words_with_frequency: list[tuple[str, float]],\n",
        "    *,\n",
        "    k: int,\n",
        ") -> float:\n",
        "    \"\"\"Score how well `word_counts` matches a language frequency list.\n",
        "\n",
        "    Approach (pragmatic and easy to explain):\n",
        "    - Take top-k language words.\n",
        "    - Compute normalized frequencies for the text restricted to those words.\n",
        "    - Compute normalized frequencies for the language restricted to those words.\n",
        "    - Return cosine similarity between the two vectors.\n",
        "\n",
        "    Notes:\n",
        "    - Score is in [0, 1] for non-negative vectors.\n",
        "    - Words not in the top-k language list are ignored in the main similarity signal (names, jargon).\n",
        "    - If there is no overlap, returns 0.0.\n",
        "    \"\"\"\n",
        "\n",
        "    if k <= 0:\n",
        "        raise ValueError(\"k must be > 0\")\n",
        "\n",
        "    topk = language_words_with_frequency[:k]\n",
        "    if not topk:\n",
        "        return 0.0\n",
        "\n",
        "    lang_map = {w: float(f) for (w, f) in topk if w}\n",
        "    # Normalize language freqs over the top-k list.\n",
        "    lang_sum = sum(lang_map.values())\n",
        "    if lang_sum <= 0:\n",
        "        return 0.0\n",
        "    lang_norm = {w: f / lang_sum for (w, f) in lang_map.items()}\n",
        "\n",
        "    # Build text frequencies over the same vocabulary.\n",
        "    text_sum = 0\n",
        "    text_map: dict[str, float] = {}\n",
        "    for w in lang_map.keys():\n",
        "        c = int(word_counts.get(w, 0))\n",
        "        if c > 0:\n",
        "            text_map[w] = float(c)\n",
        "            text_sum += c\n",
        "\n",
        "    if text_sum <= 0:\n",
        "        return 0.0\n",
        "\n",
        "    text_norm = {w: c / text_sum for (w, c) in text_map.items()}\n",
        "\n",
        "    # Cosine similarity.\n",
        "    dot = 0.0\n",
        "    a2 = 0.0\n",
        "    b2 = 0.0\n",
        "    for w in lang_map.keys():\n",
        "        a = text_norm.get(w, 0.0)\n",
        "        b = lang_norm.get(w, 0.0)\n",
        "        dot += a * b\n",
        "        a2 += a * a\n",
        "        b2 += b * b\n",
        "\n",
        "    denom = math.sqrt(a2) * math.sqrt(b2)\n",
        "    if denom <= 0:\n",
        "        return 0.0\n",
        "    return dot / denom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_language_frequency_list(language_code: str, n: int) -> list[tuple[str, float]]:\n",
        "    \"\"\"Return a list of (word, frequency) sorted by frequency desc.\n",
        "\n",
        "    Uses wordfreq if available.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        from wordfreq import top_n_list, word_frequency\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        raise RuntimeError(\n",
        "            \"wordfreq not installed. Install it (pip install wordfreq) to run this cell.\"\n",
        "        ) from exc\n",
        "\n",
        "    words = top_n_list(language_code, n)\n",
        "    return [(w, float(word_frequency(w, language_code))) for w in words]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def counts_from_text(text: str) -> dict[str, int]:\n",
        "    words = tokenize_words(text)\n",
        "    return dict(Counter(words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets\n",
        "\n",
        "Below we create 5 datasets. Replace these with real outputs from your project as needed (for example by loading JSON files produced by `--count-words`).\n",
        "\n",
        "Practical note: the assignment asks for one long wiki article (5000+ words). If you do not have one saved offline, you can still validate the scoring pipeline on synthetic or external texts, then swap in real word-count dictionaries later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "en_text = (\n",
        "    \"Team Rocket is a villainous team that tries to steal rare creatures and cause trouble. \"\n",
        "    \"They often fail, but they continue to plan and return.\"\n",
        ")\n",
        "pl_text = (\n",
        "    \"To jest przykladowy tekst po polsku. Zawiera slowa czeste i rzadkie, aby zasymulowac rozklad. \"\n",
        "    \"To nie jest tekst z wiki, ale nadaje sie do testow.\"\n",
        ")\n",
        "es_text = (\n",
        "    \"Este es un texto de ejemplo en espanol. Contiene palabras comunes y algunas menos comunes para pruebas. \"\n",
        "    \"No es un texto de wiki, pero sirve para comparar.\"\n",
        ")\n",
        "\n",
        "# Synthetic long and short samples\n",
        "wiki_long_en = counts_from_text((en_text + ' ') * 400)  # 5000+ words approx\n",
        "wiki_short_bad = counts_from_text(\"Bulbasaur Pikachu Sevii Islands Rocket-dan James Jessie Meowth \" * 5)\n",
        "ext_en = counts_from_text((\"The quick brown fox jumps over the lazy dog. \" * 300))\n",
        "ext_pl = counts_from_text((pl_text + ' ') * 300)\n",
        "ext_es = counts_from_text((es_text + ' ') * 300)\n",
        "\n",
        "datasets = {\n",
        "    'wiki_long_en': wiki_long_en,\n",
        "    'wiki_short_bad': wiki_short_bad,\n",
        "    'ext_en': ext_en,\n",
        "    'ext_pl': ext_pl,\n",
        "    'ext_es': ext_es,\n",
        "}\n",
        "{name: sum(d.values()) for name, d in datasets.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment\n",
        "\n",
        "Compute scores for k in {3, 10, 100, 1000} across 3 languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "languages = ['en', 'pl', 'es']\n",
        "ks = [3, 10, 100, 1000]\n",
        "\n",
        "language_lists: dict[str, list[tuple[str, float]]] = {}\n",
        "for lang in languages:\n",
        "    language_lists[lang] = get_language_frequency_list(lang, 5000)\n",
        "\n",
        "rows = []\n",
        "for k in ks:\n",
        "    for lang in languages:\n",
        "        lang_list = language_lists[lang]\n",
        "        for dataset_name, wc in datasets.items():\n",
        "            score = lang_confidence_score(wc, lang_list, k=k)\n",
        "            rows.append({'k': k, 'language': lang, 'dataset': dataset_name, 'score': score})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.sort_values(['k', 'dataset', 'language']).head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pivot = df.pivot_table(index=['dataset', 'k'], columns='language', values='score')\n",
        "pivot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "for dataset_name in sorted(datasets.keys()):\n",
        "    sub = df[df['dataset'] == dataset_name]\n",
        "    fig, ax = plt.subplots()\n",
        "    for lang in languages:\n",
        "        s = sub[sub['language'] == lang].sort_values('k')\n",
        "        ax.plot(s['k'], s['score'], marker='o', label=lang)\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_title(f'lang_confidence_score vs k ({dataset_name})')\n",
        "    ax.set_xlabel('k (log scale)')\n",
        "    ax.set_ylabel('score')\n",
        "    ax.grid(alpha=0.25)\n",
        "    ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes / Discussion (template)\n",
        "\n",
        "Replace this section with your own empirical observations after you swap in real wiki-derived `word-counts.json` files and real external texts.\n",
        "\n",
        "Questions to address:\n",
        "- Did the choice of languages matter?\n",
        "- Can you see evidence of inflection (many word forms) in the word-frequency overlap?\n",
        "- Was it hard to find an article that minimized the score for the wiki language?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
